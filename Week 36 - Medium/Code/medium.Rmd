---
title: "Tidy_Tuesday_Medium"
output: html_notebook
---

# Packages
```{r, warning=FALSE}
library(tidyverse)
library(tidytext)
library(SnowballC)
library(gridExtra)
library(scales)
library(lubridate)
library(gghighlight)
library(ggthemes)
library(showtext)
showtext_auto() 
data("stop_words")
```

# Load
```{r}
medium <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018-12-04/medium_datasci.csv")
```

# Glimpse
```{r}
glimpse(medium)
```

# Gather tags into 1 column
```{r}
medium %>%
  select(14:21) %>%
  gather(tag, Count) %>%
  filter(Count >=1) %>%
  select(tag)

# can't because multiple tags, not a true dummy variable
```

## TITLE WORDS
# popular words in titles with stemming
```{r}
# function
top_terms <- function(df, selectYear, ngrams) {
  if(ngrams == 2) {
  final_df <- df %>%
  mutate(date = ymd(paste0(year,"-",month,"-",day))) %>%
  filter(year(date) == selectYear) %>%
    unnest_tokens(word,title,token = 'ngrams', n = ngrams, to_lower = TRUE, drop = TRUE) %>%
    separate(word,c('word1','word2'),sep = ' ') %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word) %>%
      drop_na(word1) %>%
      drop_na(word2) %>%
      mutate(word_stem1 = wordStem(word1, language="english"),
             word_stem2 = wordStem(word2, language="english")) %>%
      unite(word, word_stem1, word_stem2, sep = ' ') %>%
      filter(!str_detect(word, "[:digit:]")) %>%
  group_by(word) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(100)
  } else {
  final_df <- df %>%
  mutate(date = ymd(paste0(year,"-",month,"-",day))) %>%
  filter(year(date) == selectYear) %>%
    unnest_tokens(word,title,token = 'ngrams', n = ngrams, to_lower = TRUE, drop = TRUE) %>%
      drop_na(word) %>%
      anti_join(stop_words) %>%
      mutate(word = wordStem(word, language="english")) %>%
      filter(!str_detect(word, "[:digit:]")) %>%
      filter(!str_detect(word, 'na')) %>%
      filter(!str_detect(word, 'de')) %>%
  group_by(word) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(100)
  }
  return(final_df)
}

# merging
title_df_2017 <- top_terms(medium, 2017, 1) %>%
  bind_rows(top_terms(medium, 2017, 2))

title_df_2018  <- top_terms(medium, 2018, 1) %>%
  bind_rows(top_terms(medium, 2018, 2))

# view
title_df_2017
title_df_2018
```

## WHAT GENERATES CLAPS?
# popular words that generate claps
```{r}
# function
clap_top_terms <- function(df, selectYear, ngrams){
  if(ngrams==2){
  final_df <- df %>%
  mutate(date = ymd(paste0(year,"-",month,"-",day))) %>%
  filter(year(date) == selectYear) %>%
      unnest_tokens(word,title,token = 'ngrams', n = ngrams, to_lower = TRUE, drop = TRUE) %>%
    separate(word,c('word1','word2'),sep = ' ') %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word) %>%
      drop_na(word1) %>%
      drop_na(word2) %>%
      mutate(word_stem1 = wordStem(word1, language="english"),
             word_stem2 = wordStem(word2, language="english")) %>%
      unite(word, word_stem1, word_stem2, sep = ' ') %>%
      filter(!str_detect(word, "[:digit:]")) %>%
    group_by(word) %>%
   summarize(total_claps = sum(claps)) %>%
  arrange(desc(total_claps)) %>%
  head(100)
  } else {
  final_df <- df %>%
  mutate(date = ymd(paste0(year,"-",month,"-",day))) %>%
  filter(year(date) == selectYear) %>%
  unnest_tokens(word,title,token = 'ngrams', n = ngrams, to_lower = TRUE, drop = TRUE) %>%
      drop_na(word) %>%
      anti_join(stop_words) %>%
      mutate(word = wordStem(word, language="english")) %>%
      filter(!str_detect(word, "[:digit:]")) %>%
      filter(!str_detect(word, 'na')) %>%
      filter(!str_detect(word, 'de')) %>%
    group_by(word) %>%
   summarize(total_claps = sum(claps)) %>%
  arrange(desc(total_claps)) %>%
  head(100)
  }
  return(final_df)
}

# merging
clap_df_2017 <- clap_top_terms(medium, 2017, 1) %>%
  bind_rows(clap_top_terms(medium, 2017, 2))

clap_df_2018 <- clap_top_terms(medium, 2018, 1) %>%
  bind_rows(clap_top_terms(medium, 2018, 2))

# view
clap_df_2017
clap_df_2018
```

# final merge (inner_join)
```{r}
title_df_2017 %>%
  inner_join(clap_df_2017) %>%
  mutate(avg_claps_generated = round(total_claps/n,2)) %>%
  arrange(desc(avg_claps_generated)) %>%
  head(50)

title_df_2018 %>%
  inner_join(clap_df_2018) %>%
  mutate(avg_claps_generated = round(total_claps/n,2)) %>%
  arrange(desc(avg_claps_generated)) %>%
  head(50)
```

## VISUALIZATION
# side-by-side comparison of popular words 
```{r}
df1 <- title_df_2017 %>%
  inner_join(clap_df_2017) %>%
  mutate(avg_claps_generated = round(total_claps/n,2)) %>%
  arrange(desc(avg_claps_generated)) %>%
  head(25) %>%
    ggplot(aes(x=reorder(word, avg_claps_generated), y=avg_claps_generated, fill = avg_claps_generated)) +
    geom_col() +
  coord_flip() +
  
  scale_fill_gradient(low = "grey80", high = "grey20") +
  scale_y_reverse() +
  scale_x_discrete(position = "top") +
  theme_light() +
  
  labs(x= "", y = "Avg. Claps Generated",
       colour = NULL,
       title = "Effective Words from 2017 & 2018",
       subtitle = "Determined using total claps by the volume of use",
       caption = "@datajake | Source: Medium.com | #TidyTuesday") +
  
  theme(panel.border = element_blank(),
        text = element_text(colour = "gray60", family = "wqy-microhei"),
        legend.position = "none",
        axis.title.x = element_text(colour="grey20", face = "bold"),
        axis.title.y = element_text(colour="grey20", face = "italic"),
        plot.title = element_text(size=15, colour = "gray20", face = "bold"),
        plot.caption = element_text(size = 10, colour = "white", face = "italic")) 

df2<- title_df_2018 %>%
  inner_join(clap_df_2018) %>%
  mutate(avg_claps_generated = round(total_claps/n,2)) %>%
  arrange(desc(avg_claps_generated)) %>%
  head(25) %>%
    ggplot(aes(x=reorder(word, avg_claps_generated), y=avg_claps_generated, fill = avg_claps_generated)) +
    geom_col() +
  coord_flip() +
  
  scale_x_discrete(position = "bottom") +
  scale_fill_gradient(low = "grey80", high = "grey20") +
  theme_light() +
  
  labs(x= "", y = "Avg. Claps Generated",
       colour = NULL,
       title = "2018",
       subtitle = "2018",
       caption = "@datajake | Source: Medium.com | #TidyTuesday") +
  
  theme(panel.border = element_blank(),
        text = element_text(colour = "gray60", family = "wqy-microhei"),
        legend.position = "none",
        axis.title.x = element_text(colour="grey20", face = "bold"),
        axis.title.y = element_text(colour="grey20", face = "italic"),
        plot.title = element_text(size=15, colour = "white", face = "bold"),
        plot.subtitle = element_text(size=15, colour = "white", face = "bold"),
        plot.caption = element_text(size = 10, colour = "gray60", face = "italic"))

grid.arrange(df1,df2, nrow=1)
```

# big difference in Years
```{r}
title_df_2017 %>%
  inner_join(clap_df_2017) %>%
  bind_rows(
    title_df_2018 %>%
  inner_join(clap_df_2018), .id="year"
  ) %>%
  mutate(avg_claps_generated = round(total_claps/n,2)) %>% 
  group_by(word) %>%
  mutate(diff=max(avg_claps_generated) - min(avg_claps_generated)) %>%
  filter(diff > 50) %>%
ggplot(aes(x = year, y = avg_claps_generated, col = word, group = word, fill = word, label = word)) +
  stat_summary(fun.y = print, geom = "line") +
  geom_text(check_overlap = T, hjust = "outward", fontface = "bold", size = 3) +
  scale_x_discrete(labels=c("2017", "2018")) +
labs(x= "", y = "Avg. Claps Generated",
       colour = NULL,
       title = "Highlighting Greater Shifts in Popular Words",
       subtitle = "Words with large differences in average claps (greater than 50) are shown",
       caption = "@datajake | Source: Medium.com | #TidyTuesday") +
  theme_calc() +
  theme(panel.border = element_blank(),
        text = element_text(colour = "gray60", family = "wqy-microhei"),
        legend.position = "none",
        axis.title.x = element_text(colour="grey20", face = "bold"),
        axis.title.y = element_text(colour="grey20", face = "italic"),
        plot.title = element_text(size=15, colour = "gray20", face = "bold"),
        plot.caption = element_text(size = 10, colour = "gray60", face = "italic"),
        plot.margin = margin(t=10, r=20, b=10, l=10, unit = "pt")) 

```

# DataCritics articles
```{r}
medium %>%
  filter(author == "DataCritics") %>%
  View()
```

# drob's net  
```{r}
theme_set(theme_light())

#cleaning
medium %>%
  summarize_at(vars(starts_with("tag_")), sum)

medium_gathered <- medium %>%
  gather(tag,value, starts_with("tag_")) %>%
  mutate(tag = str_remove(tag, "tag_")) %>%
  filter(value == 1)

# counts by tag
medium_gathered %>%
  count(tag, sort = T)

# claps
medium_gathered %>%
  group_by(tag) %>%
  summarize(median_claps = median(claps)) %>%
  arrange(desc(median_claps))

# reading time visual
medium %>%
  mutate(reading_time = pmin(10, reading_time)) %>%
  ggplot(aes(reading_time)) +
  geom_histogram(binwidth = 0.5) +
  scale_x_continuous(breaks = seq(2,10,2),
                     labels = c(seq(2,8,2), "10+")) + 
  labs(x="Typical Medium Reading Time")

# average reading time
medium_gathered %>%
  group_by(tag) %>%
  summarize(reading_time = mean(reading_time)) %>%
  arrange(desc(reading_time))

```

### text mining
```{r}
library(tidytext)

# cleaning
medium_words <- medium %>%
  filter(!is.na(title)) %>%
  select(title, subtitle, year, reading_time, claps) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by= "word") %>%
  filter(!word %in% c("de", "en", "la","para")) %>%
  filter(str_detect(word, "[a-z]"))

# visual
medium_words %>%
  count(word, sort = T)  %>%
  mutate(word = fct_reorder(word,n)) %>%
  head(20) %>%
  ggplot(aes(x= word,y= n)) +
  geom_col() + 
  coord_flip() +
  labs(title = "Common words in Medium Post Titles")
```

```{r}
# how many times does this word appear in dataset
medium_words %>%
  add_count(word) %>%
  filter(n >= 500) %>%
  count(word)

# buzz words that suck
medium_words %>%
  add_count(word) %>%
  filter(n >= 500) %>%
  group_by(word) %>%
  summarize(median_claps=  median(claps),
            occurances = n()) %>%
  arrange(median_claps)

# geometric means distinguishes 1.0 from each other, since claps can be 0, we + 1 then -1
tag_claps <- medium_words %>%
  add_count(word) %>%
  filter(n >= 500) %>%
  group_by(word) %>%
  summarize(median_claps=  median(claps),
            geometric_mean_claps = exp(mean(log(claps + 1))) - 1,
            occurences = n()) %>%
  arrange(median_claps)

# reworking medium words with TRANSMUTE and adding grouping row numbers
medium_words <- medium %>%
  filter(!is.na(title)) %>%
  transmute(post_id = row_number(), title, subtitle, year, reading_time, claps) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by= "word") %>%
  filter(!word %in% c("de", "en", "la","para")) %>%
  filter(str_detect(word, "[a-z]"))


# tidytext chapter 4.2
medium_words_filtered <- medium_words %>%
  add_count(word) %>%
  filter(n >= 250)


# pairs of words by correlations
library(widyr)

top_word_cors <- medium_words_filtered %>%
  select(post_id, word) %>%
  pairwise_cor(word, post_id, sort=T) %>%
  head(150)

library(ggraph)
library(igraph)

# visual of clusters
set.seed(2018)
top_word_cors %>%
  graph_from_data_frame() %>%
  ggraph() +
  geom_edge_link() + 
  geom_node_point() +
  geom_node_text(aes(label = name), repel = T) +
  theme_void()


# adding another dimension
tag_claps <- medium_words_filtered %>%
  add_count(word) %>%
  group_by(word) %>%
  summarize(median_claps = median(claps),
            geometric_mean_claps = exp(mean(log(claps + 1))) - 1,
            occurences = n()) %>%
  arrange(desc(median_claps))

vertices <- tag_claps %>%
  filter(word %in% top_word_cors$item1 | word %in% top_word_cors$item2)

# average claps earned
top_word_cors %>%
  graph_from_data_frame(vertices = vertices) %>%
  ggraph() +
  geom_edge_link() + 
  geom_node_point(aes(size = occurences * 1.1)) +
  geom_node_point(aes(size = occurences, color = geometric_mean_claps)) +
  geom_node_text(aes(label = name), repel = T) +
  theme_void() +
  labs(title="What's hot and what's not in Medium article titles?", 
       subtitle="Colour shows the geometric mean of # of claps on articles with this word in the title", color = "Avg. Claps", size = "Occurences") +
  scale_color_gradient2(low = "blue",
                        high = "red",
                        midpoint = 10) +
  theme(text = element_text(colour = "gray60", family = "wqy-microhei"),
        plot.title = element_text(size=15, colour = "gray20", face = "bold"),
        plot.caption = element_text(size = 10, colour = "gray60", face = "italic"),
    plot.margin = margin(t=10, r=20, b=10, l=10, unit = "pt")) 

```
### predicting the number of claps based on title and tag - see how much the word contributes
```{r}
# turn this into a sparse matrix
medium_words_filtered %>%
  distinct(post_id, word, claps)


post_word_matrix <- medium_words_filtered %>%
  distinct(post_id, word, claps) %>%
  cast_sparse(post_id, word)

# 164 variables, 58000 rows
dim(post_word_matrix)
# some don't pop up
rownames(post_word_matrix)


# LASSO REGRESSION MODEL
library(glmnet)

# have to reclean this
medium_words <- medium %>%
  filter(!is.na(title)) %>%
  select(title, subtitle, year, reading_time, claps) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by= "word") %>%
  filter(!word %in% c("de", "en", "la","para")) %>%
  filter(str_detect(word, "[a-z]"))

medium_processed <- medium %>%
  select(-x1) %>%
  mutate(post_id = row_number())

medium_words <- medium_processed %>%
  filter(!is.na(title)) %>%
  select(post_id, title, subtitle, year, reading_time, claps) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by= "word") %>%
  filter(!word %in% c("de", "en", "la","para")) %>%
  filter(str_detect(word, "[a-z]"))

# matching rows to make the y 
claps <- medium_processed$claps[match(rownames(post_word_matrix), medium_processed$post_id)]

lasso_model <- cv.glmnet(post_word_matrix, log(claps + 1))

```

```{r}
# as we decrease our penalty we approach a better MSE
plot(lasso_model)

library(broom) 
# add coefficients as they appear by term
tidy(lasso_model$glmnet.fit)

# INFLUENCE
# learning gets bigger and has positive influence, then hadoop has negative influence, then startup, then deep
# can see negative influences and positive 
tidy(lasso_model$glmnet.fit) %>%
  filter(term %in% c("learning", "hadoop", "gdpr", "deep", "startup", "marketing")) %>%
  ggplot(aes(x = lambda, y = estimate, colour = term)) +
  geom_line() +
  scale_x_log10()

# can use lambda.min to use best lambda value for MSE, can see the top arre positive, bottom are negative
tidy(lasso_model$glmnet.fit) %>%
  filter(lambda == lasso_model$lambda.min) %>%
  arrange(desc(estimate))
```

```{r}

```

